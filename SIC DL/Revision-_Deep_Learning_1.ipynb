{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Revision-_Deep_Learning_1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"C78ysjnnhLAJ"},"source":["# Revision du test 601!!! "]},{"cell_type":"markdown","metadata":{"id":"nIh-q_cmhLAP"},"source":["## \"NLP Classification Analysis.\""]},{"cell_type":"markdown","metadata":{"id":"YD7Kxnq7hLAQ"},"source":["#### Answer the following questions by providing Python code:\n","#### Objectives:\n","- Pre-processing of text data.\n","- Create a TF IDF representation.\n","- Build network and train it untill validation loss reduces (EarlyStopping)\n","- Carry out the predictive analysis using the Naive Bayes algorithm. \n","- Carry out the predictive analysis using SVM. "]},{"cell_type":"code","metadata":{"id":"PvixsqYvhLAS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630343019440,"user_tz":-60,"elapsed":3792,"user":{"displayName":"NOUR MERIEM BOUAYED","photoUrl":"","userId":"12911441041777911065"}},"outputId":"62968132-49c1-4878-d36a-656ea1d2de91"},"source":["import re\n","\n","# Importing required libraries\n","from sklearn.model_selection import train_test_split\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","# keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","from keras.layers.core import Reshape, Flatten\n","from keras.callbacks import EarlyStopping\n","from keras.optimizers import Adam    \n","    # Turn the warnings off."],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_9D1OsUrhLAU"},"source":["## Carry out the data pre-processing\n","1). Read in data\n"]},{"cell_type":"code","metadata":{"id":"_3m-wuWVfmPT"},"source":[" # defining function to clean text and retrive closs-validation datasets\n","def cleantxt(txt):\n","    \"\"\"\n","    Cleans the string passed. Cleaning Includes-\n","    1. remove special characters/symbols\n","    2. convert text to lower-case\n","    3. retain only alphabets\n","    4. remove words less than 3 characters\n","    5. remove stop-words\n","    \"\"\"  \n","    # collecting english stop words from nltk-library\n","    stpw = stopwords.words('english')\n","    \n","    # Adding custom stop-words\n","    stpw.extend(['www','http','utc'])\n","    stpw = set(stpw)\n","    \n","    # using regex to clean the text\n","    txt = re.sub(r\"\\n\", \" \", txt)\n","    txt = re.sub(\"[\\<\\[].*?[\\>\\]]\", \" \", txt)\n","    txt = txt.lower()\n","    txt = re.sub(r\"[^a-z ]\", \" \", txt)\n","    txt = re.sub(r\"\\b\\w{1,3}\\b\", \" \",txt)\n","    txt = \" \".join([x for x in txt.split() if x not in stpw])\n","    return txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"iBLSt5InlNdg","executionInfo":{"status":"error","timestamp":1630343020324,"user_tz":-60,"elapsed":893,"user":{"displayName":"NOUR MERIEM BOUAYED","photoUrl":"","userId":"12911441041777911065"}},"outputId":"c7b492fe-0ee2-4a05-8089-16670b5ba14e"},"source":["import pandas as pd\n","train=pd.read_csv('train.csv')\n","X_test=pd.read_csv('test.csv')\n","Y_test=pd.read_csv('test_labels.csv')\n","\n","train.drop('id',axis=1,inplace=True)\n","X_test.drop('id',axis=1,inplace=True)\n","Y_test.drop('id',axis=1,inplace=True)\n","\n","train.shape\n","X_test.shape"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-5e40611eee3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"]}]},{"cell_type":"code","metadata":{"id":"uT05sLeJsM6M"},"source":["X_train=train.iloc[:-20000,0]\n","X_val=train.iloc[-20000:,0]\n","Y_train=train.iloc[:-20000,1:]\n","Y_val=train.iloc[-20000:,1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKk5yrPghLAZ"},"source":["import numpy as np\n","np.unique(Y_train, return_counts=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r6gTQeKqqmoQ"},"source":["we have unbalanced classes"]},{"cell_type":"code","metadata":{"id":"N962Q4EphLAa"},"source":["# Visualize the response variable, according to one of the ratget values\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.countplot(Y_train.iloc[:,4]).set_title(\"Frequency Table\") # replace 4 by 3 then 5\n","plt.show()\n","sns.countplot(Y_train.iloc[:,3]).set_title(\"Frequency Table\") # replace 4 by 3 then 5\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xokt070xhLAd"},"source":["2). Create a TF IDF representation.\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","\n","https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n","\n"]},{"cell_type":"code","metadata":{"id":"JYvem45XhLAe"},"source":["vect = TfidfVectorizer(decode_error='ignore',stop_words='english')\n","train_tfidf = vect.fit_transform(X_train)\n","val_tfidf = vect.transform(X_val)\n","test_tfidf = vect.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7OYrR2JTfmPh"},"source":["## 2)-Build network and train it untill validation loss reduces (EarlyStopping)\n"]},{"cell_type":"code","metadata":{"id":"gJ3TGK3PfmPh"},"source":["ip_dim = train_tfidf.shape[1]\n","\n","model = Sequential() #define a sequential model\n","model.add(Dense(64,input_dim=ip_dim,activation='relu'))  #Define a first layer with 64 units and relu as an activation function\n","model.add(Dense(64,activation='relu')) #Define a second layer with 64 units and relu as an activation function\n","model.add(Dense(6,activation='sigmoid')) # Define the input layer with 6units and sigmoid activation function"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rf9XoKhefmPl","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1630361135152,"user_tz":-60,"elapsed":26,"user":{"displayName":"NOUR MERIEM BOUAYED","photoUrl":"","userId":"12911441041777911065"}},"outputId":"ba118e1f-dc89-4b93-ea27-52469af7e324"},"source":["\n","#Compiling Model using optimizer with lr=1e-3\n","opt = Adam(lr=1e-3) \n","#model.compile by using loss='binary_crossentropy' and Adam optimizer\n","\n","model.compile(loss='binary_crossentropy',optimizer=opt)\n","\n","# Fitting Model to the data\n","\n","callbacks = [EarlyStopping(monitor='val_loss')]\n","#hist_adam = model.fit(train_tfidf, np.asarray(y_train), batch_size=1000, epochs=3, verbose=2, validation_data=(val_tfidf, np.asarray(y_val)),callbacks=callbacks)  # starts training\n","\n","#import tensorflow as tf\n","#tf.sparse.reorder(train_tfidf)\n","#tf.sparse.reorder(Y_train)\n","#tf.sparse.reorder(val_tfidf)\n","#tf.sparse.reorder(Y_val)\n","\n","hist_adam = model.fit(train_tfidf, np.asarray(Y_train), batch_size=1000, epochs=3, verbose=2, validation_data=(val_tfidf, np.asarray(Y_val)),\n","         callbacks=callbacks)  # starts training"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9afbe4ddf391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Compiling Model using optimizer with lr=1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.compile by using loss='binary_crossentropy' and Adam optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"]}]},{"cell_type":"code","metadata":{"id":"_T2Bb_6DfmPn"},"source":["#plot the following !!! \n","plt.suptitle('Optimizer : Adam', fontsize=10)\n","#..\n","#..\n","#..\n","\n","#.."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E0j9Fo2xhLAe"},"source":["## 3)Apply the Naive Bayes algorithm. Calculate the accuracy."]},{"cell_type":"code","metadata":{"id":"7zR71TuWfmPo"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","vect = CountVectorizer(decode_error='ignore',stop_words='english')\n","train_dtm=vect.fit_transform(X_train)\n","train_dtm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cukO1LXWfmPp"},"source":["from sklearn.naive_bayes import MultinomialNB\n","nb = #apply MultinomialNB\n","nb.fit(train_dtm, y_train.severe_toxic)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRM7jPG3fmPq"},"source":["MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6j90GlMcfmPr"},"source":["y_pred_nb = nb.predict(train_dtm)\n","from sklearn import metrics\n","\n","#find the accuracy"],"execution_count":null,"outputs":[]}]}